{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tensorflow==1.14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import datetime\n",
    "import time\n",
    "import collections \n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Preprocess Data\n",
    "\n",
    "After reading the data we will perform a few pre-processing steps:\n",
    "\n",
    "1. Remove the products with less than 100 reviews. This is a simple heuristic to both reduce the training set size and make sure that we have enough data to \"synthetize\" a super-review.\n",
    "2. Make sure we don't have too many word not in embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data'\n",
    "fname = 'amazon_reviews_us_Musical_Instruments_v1_00.tsv'\n",
    "\n",
    "os.path.join(path, fname)\n",
    "df = pd.read_csv(os.path.join(path, fname), sep = '\\t', error_bad_lines = False, warn_bad_lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We started with 123284 unique ids\n",
      "After cleansing we are left with 1105 products\n",
      "Total dataset size is now 286055 rows\n"
     ]
    }
   ],
   "source": [
    "# Find the products with less than 100 reviews\n",
    "print(\"We started with {} unique ids\".format(len(df['product_id'].unique())))\n",
    "reviews_threshold = 100\n",
    "pdist = df[['product_id', 'review_id']].groupby('product_id', as_index = False).count()\n",
    "prods_to_keep = pdist[pdist['review_id'] > reviews_threshold]['product_id'].unique()\n",
    "\n",
    "df = df[df['product_id'].isin(prods_to_keep)]\n",
    "print(\"After cleansing we are left with {} products\".format(len(df['product_id'].unique())))\n",
    "print(\"Total dataset size is now {} rows\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_and_labels(df):\n",
    "  \"\"\"Creates X vecotr and y label from reviews\n",
    "  dataset\n",
    "  \n",
    "  Arguments:\n",
    "    df: pandas dataframe\n",
    "    \n",
    "   Return:\n",
    "    texts: vector of reviews \n",
    "    labels: binary vector\n",
    "    id: binary vector\n",
    "  \n",
    "  \"\"\"\n",
    "  texts = []\n",
    "  labels = []\n",
    "  ids = []\n",
    "  prod_ids = []\n",
    "  \n",
    "  for i in df.itertuples():\n",
    "    texts.append(str(i.review_body))\n",
    "    ids.append(i.review_id)\n",
    "    prod_ids.append(i.product_id)\n",
    "  \n",
    "    if i.star_rating in (1,2,3):\n",
    "      labels.append(0)\n",
    "    else:\n",
    "      labels.append(1)\n",
    "   \n",
    "  assert len(texts) == len(labels) == len(ids)\n",
    "  \n",
    "  return texts, labels, ids, prod_ids\n",
    "\n",
    "# Create Text Labels and Id vectors\n",
    "texts, labels, ids, prod_ids = create_text_and_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in contractions, needed to clean up text\n",
    "outpath = 'Data'\n",
    "with open(os.path.join(outpath, 'contractions.pickle'), \"rb\") as f:\n",
    "  contractions = pickle.load(f)\n",
    "\n",
    "def clean_text(text, contractions, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'<br  >', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text.rstrip()\n",
    "\n",
    "# Clean up words using function defined above\n",
    "cleaned_text = []\n",
    "\n",
    "for t in texts:\n",
    "  \n",
    "  new_t = clean_text(t, contractions, remove_stopwords = False)\n",
    "  cleaned_text.append(new_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.8 s, sys: 26.9 ms, total: 22.9 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenize reviews, using only a limited amount of words.\n",
    "# NOTE: the tokenizer ignores words that are not in the vocubalary, without putting a placeholder\n",
    "max_words = 100000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(cleaned_text)\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_text)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.58 s, sys: 845 ms, total: 8.43 s\n",
      "Wall time: 8.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_glove_embedding(path):\n",
    "  \n",
    "  embedding_index = {}\n",
    "\n",
    "  with open(path) as f:\n",
    "  \n",
    "    for line in f:\n",
    "    \n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      coefs = np.array(values[1:])\n",
    "      embedding_index[word] = coefs\n",
    "      \n",
    "  return embedding_index\n",
    "\n",
    "# Import Glove embedding\n",
    "path = 'Data'\n",
    "glove_path = 'glove.6B.100d.txt'\n",
    "\n",
    "embedding_index = get_glove_embedding(os.path.join(path, glove_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There ere 37084 unique words that cannot be found in the embedding\n",
      "Word : m50s ||  Frequency:  890\n",
      "Word : m50x ||  Frequency:  871\n",
      "Word : h4n ||  Frequency:  773\n",
      "Word : fiio ||  Frequency:  663\n",
      "Word : sennheisers ||  Frequency:  635\n",
      "Word : 7506 ||  Frequency:  563\n",
      "Word : recomend ||  Frequency:  536\n",
      "Word : sm58 ||  Frequency:  511\n",
      "Word : at2020 ||  Frequency:  490\n",
      "Word : earpads ||  Frequency:  444\n",
      "Word : beyerdynamic ||  Frequency:  432\n",
      "Word : lavalier ||  Frequency:  427\n",
      "Word : mxl ||  Frequency:  406\n",
      "Word : videoid ||  Frequency:  375\n",
      "Word : shockmount ||  Frequency:  359\n",
      "Word : hitlights ||  Frequency:  344\n",
      "Word : hd650 ||  Frequency:  337\n",
      "Word : ad700 ||  Frequency:  335\n",
      "Word : excelent ||  Frequency:  334\n",
      "Word : ukes ||  Frequency:  334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedded_words = set(embedding_index.keys())\n",
    "target_words = set(word_index.keys())\n",
    "\n",
    "words_diff = list(target_words.difference(embedded_words))\n",
    "print(\"There ere {} unique words that cannot be found in the embedding\".format(len(words_diff)))\n",
    "\n",
    "# Find the top words not in the embedding\n",
    "no_embedding = sorted([(word_index[x], tokenizer.word_counts[x]) for x in words_diff])\n",
    "\n",
    "# A few words won't have an embedding, let's check the top 20\n",
    "for ne in no_embedding[:20]:\n",
    "  print(\"Word :\", tokenizer.index_word[ne[0]], \"||  Frequency: \", ne[1])\n",
    "\n",
    "# TODO - we can think at manually cleaning mispelled words like 'recomend', excellet', 'awsome'\n",
    "pd.Series([x[1] for x in no_embedding]).describe()\n",
    "np.percentile([x[1] for x in no_embedding], 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embedding Matrix\n",
    "\n",
    "embedding_dim = 100\n",
    "max_words = len(word_index.keys())+1\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "  \n",
    "  if i < max_words:\n",
    "\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4491\n",
      "95% of the data have at most 195.0 words\n"
     ]
    }
   ],
   "source": [
    "# Find a parameter to decide how many words we take in\n",
    "\n",
    "length_dist = [len(x) for x in sequences]\n",
    "print(max(length_dist))\n",
    "print(\"95% of the data have at most {} words\".format(np.percentile(length_dist, 95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.06 s, sys: 11.8 ms, total: 2.07 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maxlen = 200\n",
    "# Pad data to normalize sequences to be 200 words (captures 95% of reviews)\n",
    "data = pad_sequences(sequences, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomnly Split Data based on product ids\n",
    "# We will use reviews of 80% of the eligible products to build the model\n",
    "# The rest to evaluate and test shap explainer\n",
    "np.random.seed(55)\n",
    "train_prods = np.random.choice(prods_to_keep, size = int(len(prods_to_keep)*0.7), replace = False)\n",
    "test_prods = list(set(prods_to_keep).difference(set(train_prods)))\n",
    "assert len(train_prods) + len(test_prods) == len(prods_to_keep)\n",
    "\n",
    "indices = [idx if i in train_prods else -1 for idx, i in enumerate(prod_ids)]\n",
    "train_indices = np.array(list(filter(lambda x: x != -1, indices)))\n",
    "indices_test = [idx if i not in train_prods else -1 for idx, i in enumerate(prod_ids)]\n",
    "test_indices = np.array(list(filter(lambda x: x != -1, indices_test)))\n",
    "assert len(train_indices) + len(test_indices) == len(prod_ids)\n",
    "\n",
    "x_train = data[train_indices]\n",
    "y_train = np.array(labels)[train_indices]\n",
    "ids_train = np.array(ids)[train_indices]\n",
    "prod_ids_train = np.array(prod_ids)[train_indices]\n",
    "\n",
    "x_test =  data[test_indices]\n",
    "y_test =  np.array(labels)[test_indices]\n",
    "ids_test = np.array(ids)[test_indices]\n",
    "prod_ids_test = np.array(prod_ids)[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save numpy arrays\n",
    "outpath = 'Data'\n",
    "\n",
    "np.save(os.path.join(outpath, 'x_train'), x_train)\n",
    "np.save(os.path.join(outpath, 'y_train'), y_train)\n",
    "np.save(os.path.join(outpath, 'ids_train'), ids_train)\n",
    "np.save(os.path.join(outpath, 'prod_ids_train'), prod_ids_train)\n",
    "\n",
    "np.save(os.path.join(outpath, 'x_test'), x_test)\n",
    "np.save(os.path.join(outpath, 'y_test'), y_test)\n",
    "np.save(os.path.join(outpath, 'ids_test'), ids_test)\n",
    "np.save(os.path.join(outpath, 'prod_ids_test'), prod_ids_test)\n",
    "\n",
    "with open(os.path.join(outpath, 'dictionary.pickle'), \"wb\") as output_file:\n",
    "  pickle.dump(tokenizer, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "\n",
    "def lstm_model(max_words, embedding_dim, input_length):\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_words, embedding_dim, input_length=input_length))\n",
    "  model.add(LSTM(100, return_sequences = False)) \n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default we set the embedding matrix not to be trainable\n",
    "model = lstm_model(max_words, embedding_dim, maxlen)\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"Models/weights-lstm-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=False, mode='max')\n",
    "\n",
    "# Tensorboard logs\n",
    "log_dir=\"Logs/lstm\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [checkpoint, tensorboard_callback]\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a simpler fully connected model to use as a benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_model(max_words, embedding_dim, input_length):\n",
    "    \"\"\"First model to benchmark is a fully connected model\n",
    "  \n",
    "      Arguments:\n",
    "        embedding_dim: size of the embedding space\n",
    "        max_words: rows of the embedding matrix\n",
    "        input_length: max length of text\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='tanh'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default we set the embedding matrix not to be trainable\n",
    "model = fully_connected_model(max_words, embedding_dim, maxlen)\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# checkpoint\n",
    "filepath = \"Models/weights-fullyconnected-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,  verbose=1, save_best_only=False, mode='max')\n",
    "\n",
    "# Tensorboard logs\n",
    "log_dir = \"Logs/fullyconnected\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [tensorboard_callback]\n",
    "\n",
    "history = model.fit(x_train, \n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
